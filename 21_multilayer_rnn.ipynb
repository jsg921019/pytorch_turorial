{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fe161a88570>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = (\"if you want to build a ship, don't drum up people together to \"\n",
    "            \"collect wood and don't assign them tasks and work, but rather \"\n",
    "            \"teach them to long for the endless immensity of the sea.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder:\n",
    "    \n",
    "    def __init__(self, sentence):\n",
    "        self.char_list = list(set(sentence))\n",
    "        self.size = len(self.char_list)\n",
    "        self.char_dict = {c:i for i, c in enumerate(self.char_list)}\n",
    "        self.eye = np.eye(self.size)\n",
    "    \n",
    "    def int_encode(self, seq):\n",
    "        return [self.char_dict[c] for c in seq]\n",
    "\n",
    "    def one_hot_encode(self, seq):\n",
    "        return self.eye[self.int_encode(seq)]\n",
    "    \n",
    "    def decode(self, seq):\n",
    "        return ''.join([self.char_list[i] for i in seq])\n",
    "\n",
    "encoder = Encoder(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = 10\n",
    "\n",
    "x_data = []\n",
    "y_data = []\n",
    "\n",
    "for i in range(len(sentence) - sequence_length):\n",
    "    x_data.append(encoder.one_hot_encode(sentence[i:i + sequence_length]))\n",
    "    y_data.append(encoder.int_encode(sentence[i+1: i+sequence_length+1]))\n",
    "    \n",
    "X = torch.FloatTensor(x_data)\n",
    "Y = torch.LongTensor(y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLayerRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim, layers):\n",
    "        super(MultiLayerRNN, self).__init__()\n",
    "        self.rnn = nn.RNN(input_dim, hidden_dim, num_layers=layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "    def forward(self, X):\n",
    "        out, _ = self.rnn(X)\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/rnn_layers.png\" width=40% />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = encoder.size\n",
    "hidden_size = encoder.size\n",
    "n_layers = 2\n",
    "n_epochs = 250\n",
    "learning_rate = 0.1\n",
    "\n",
    "model = MultiLayerRNN(input_size, hidden_size, n_layers)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "epoch 50  loss:0.4099\n",
      "ipkyou want to build a ship, don't drum up people together to collect wood and don't dssign them tasks and work, but rather teach them ta long for the endless immensity of the sea.\n",
      "---\n",
      "epoch 100 loss:0.2528\n",
      "it you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "---\n",
      "epoch 150 loss:0.2587\n",
      "ig you want to build a ship, don't arum up people together to collect wood and don't assign them tosks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "---\n",
      "epoch 200 loss:0.2344\n",
      "if you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "---\n",
      "epoch 250 loss:0.2317\n",
      "it you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, n_epochs+1):\n",
    "    \n",
    "    y_pred = model(X).view(-1, hidden_size)\n",
    "    loss = criterion(y_pred, Y.view(-1))\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch%50 == 0:\n",
    "        results = y_pred.argmax(dim=1).tolist()\n",
    "        predict_str = results[:10] + results[19::10]\n",
    "        print('---')\n",
    "        print(f'epoch {epoch:<4}loss:{loss.item():.4f}')\n",
    "        print('i' + encoder.decode(predict_str))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
